{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdlDCZOkPOde85TzPa5zM4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ch23s020/Assignment3/blob/main/Assignment3_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gju5J-AqPMBq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7634ac5b-a318-438d-a7b4-ca79ec18fb85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.2.0-py2.py3-none-any.whl (281 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.1/281.1 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.2.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import gdown\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb"
      ],
      "metadata": {
        "id": "VRxChUlBgx5P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download URLs for Google Sheets as CSV. Separated Marathi Files into three different folders as Training , Validation and Test\n",
        "\n",
        "train_url = \"https://docs.google.com/spreadsheets/d/11duz5Vbqay5TVn_uyglVQVcEZllTbWQt_8zTt2TcBSA/export?format=csv\"\n",
        "\n",
        "valid_url = \"https://docs.google.com/spreadsheets/d/1KbKFfxFkMddkZde0r5PWKnQ0vzdh-XihxsMP7XUFDJc/export?format=csv\"\n",
        "\n",
        "test_url = \"https://docs.google.com/spreadsheets/d/1ItKDweGPNtzWiF3rs0jzKjh7ZRRkas2hz7yWvbt4yzQ/export?format=csv\"\n",
        "\n",
        "# Paths to save the files\n",
        "\n",
        "train_output = 'train_data.csv'\n",
        "\n",
        "valid_output = 'valid_data.csv'\n",
        "\n",
        "test_output = 'test_data.csv'\n",
        "\n",
        "# Downloading the files\n",
        "\n",
        "gdown.download(train_url, train_output, quiet=False)\n",
        "\n",
        "gdown.download(valid_url, valid_output, quiet=False)\n",
        "\n",
        "gdown.download(test_url, test_output, quiet=False)\n"
      ],
      "metadata": {
        "id": "P5Ak6TBNsM9O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "f2d2bc6f-d176-4319-9d8c-2aaedc1390b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=None\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://docs.google.com/spreadsheets/d/11duz5Vbqay5TVn_uyglVQVcEZllTbWQt_8zTt2TcBSA/export?format=csv\n",
            "To: /content/train_data.csv\n",
            "2.23MB [00:00, 6.07MB/s]\n",
            "Downloading...\n",
            "From: https://docs.google.com/spreadsheets/d/1KbKFfxFkMddkZde0r5PWKnQ0vzdh-XihxsMP7XUFDJc/export?format=csv\n",
            "To: /content/valid_data.csv\n",
            "143kB [00:00, 8.18MB/s]\n",
            "Downloading...\n",
            "From: https://docs.google.com/spreadsheets/d/1ItKDweGPNtzWiF3rs0jzKjh7ZRRkas2hz7yWvbt4yzQ/export?format=csv\n",
            "To: /content/test_data.csv\n",
            "149kB [00:00, 8.76MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'test_data.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data laod and  Pre-Processing"
      ],
      "metadata": {
        "id": "V02Ijr_i51DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load data\n",
        "\n",
        "def load_data(file_path):\n",
        "\n",
        "    data = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
        "\n",
        "        csvreader = csv.reader(csvfile)\n",
        "\n",
        "        for idx, row in enumerate(csvreader):\n",
        "\n",
        "            try:\n",
        "\n",
        "                x = str(row[0])  # first column contains Romanized strings\n",
        "\n",
        "                y = str(row[1])  # second column contains Devanagari strings\n",
        "\n",
        "                data.append((x, y))\n",
        "\n",
        "            except IndexError:\n",
        "\n",
        "                print(f\"IndexError in row {idx + 1}: {row}\")\n",
        "                # To resolve and verify the index error\n",
        "\n",
        "    return data\n",
        "\n",
        "# Load the data\n",
        "\n",
        "train_data = load_data(train_output)\n",
        "\n",
        "valid_data = load_data(valid_output)\n",
        "\n",
        "test_data = load_data(test_output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print a sample from each dataset to verify\n",
        "\n",
        "print(\"Sample from train data:\", train_data[0])\n",
        "\n",
        "print(\"Sample from valid data:\", valid_data[0])\n",
        "\n",
        "print(\"Sample from test data:\", test_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4RqmgqT5wxk",
        "outputId": "996d99d8-e7c5-4b6c-bb3e-9abc631fa596"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample from train data: ('fusharun', 'फुशारुन')\n",
            "Sample from valid data: ('garvyabarobarach', 'गारव्याबरोबरच')\n",
            "Sample from test data: ('heetler', 'हिटलर')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN\n"
      ],
      "metadata": {
        "id": "IVZ9ScpG7SHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, char2index, max_length=20):\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "        self.char2index = char2index\n",
        "\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        x, y = self.data[idx]\n",
        "\n",
        "        x_indices = [self.char2index[c] for c in x] + [self.char2index['<PAD>']] * (self.max_length - len(x))\n",
        "\n",
        "        y_indices = [self.char2index[c] for c in y] + [self.char2index['<PAD>']] * (self.max_length - len(y))\n",
        "\n",
        "        return torch.tensor(x_indices), torch.tensor(y_indices), len(x), len(y)\n",
        "\n",
        "def collate_fn(batch):\n",
        "\n",
        "    x, y, x_lengths, y_lengths = zip(*batch)\n",
        "\n",
        "    x = torch.nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=char2index['<PAD>'])  #P\n",
        "\n",
        "    y = torch.nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=char2index['<PAD>'])\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# Create character to index mappings\n",
        "\n",
        "all_chars = sorted(set(''.join([x for x, y in train_data + valid_data + test_data]) + ''.join([y for x, y in train_data + valid_data + test_data])))\n",
        "\n",
        "char2index = {char: idx for idx, char in enumerate(all_chars)}\n",
        "\n",
        "char2index['<PAD>'] = len(char2index)\n",
        "\n",
        "char2index['<SOS>'] = len(char2index) + 1\n",
        "\n",
        "char2index['<EOS>'] = len(char2index) + 2\n"
      ],
      "metadata": {
        "id": "HIYqFOCZ7WDW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n"
      ],
      "metadata": {
        "id": "Zo2eGG7v7xk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb\n",
        "\n",
        "sweep_config = {\n",
        "\n",
        "    \"method\": \"random\",\n",
        "\n",
        "    \"parameters\": {\n",
        "\n",
        "        \"learning_rate\": {\"values\": [0.001, 0.01, 0.1]},\n",
        "        \"batch_size\": {\"values\": [32]},\n",
        "        \"num_epochs\": {\"values\": [5, 10, 15, 20, 40, 60]},\n",
        "        \"encoder_layers\": {\"values\": [1, 2, 3]},\n",
        "        \"decoder_layers\": {\"values\": [1, 2, 3]},\n",
        "        \"hidden_dim\": {\"values\": [128, 256, 512]},\n",
        "        \"embedding_dim\": {\"values\": [128, 256, 512]},\n",
        "        \"dropout_rate\": {\"values\": [0, 0.1, 0.2]},\n",
        "        \"rnn_cell_type\": {\"values\": [\"lstm\", \"rnn\", \"gru\"]},\n",
        "        \"bidirectional\": {\"values\": [False]},\n",
        "        \"max_length\": {\"values\": [20, 60, 100, 150]},\n",
        "        \"gradient_clip\": {\"values\": [1, 2]},\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"transliteration-Assign3\")\n",
        "\n",
        "\n",
        "# Datasets and Dataloaders\n",
        "\n",
        "train_dataset = TransliterationDataset(train_data, char2index)\n",
        "\n",
        "valid_dataset = TransliterationDataset(valid_data, char2index)\n",
        "\n",
        "test_dataset = TransliterationDataset(test_data, char2index)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q28FCbJ471Gs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "4954e7a2-ded2-464d-f16b-227ffd7815d5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: e9i15px2\n",
            "Sweep URL: https://wandb.ai/ch23s020/transliteration-Assign3/sweeps/e9i15px2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Components and Classes, Training Function\n",
        "\n",
        "class EmbeddingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embedding_dim):\n",
        "\n",
        "        super(EmbeddingLayer, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.embedding(x)\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, dropout, rnn_type='lstm', bidirectional=False):\n",
        "\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        self.embedding = EmbeddingLayer(input_dim, embedding_dim)\n",
        "\n",
        "        rnn_cls = {'rnn': nn.RNN, 'lstm': nn.LSTM, 'gru': nn.GRU}[rnn_type]\n",
        "\n",
        "        self.rnn = rnn_cls(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        outputs, hidden = self.rnn(x)\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers, dropout, rnn_type='lstm', bidirectional=False):\n",
        "\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.embedding = EmbeddingLayer(output_dim, embedding_dim)\n",
        "\n",
        "        rnn_cls = {'rnn': nn.RNN, 'lstm': nn.LSTM, 'gru': nn.GRU}[rnn_type]\n",
        "\n",
        "        self.rnn = rnn_cls(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "\n",
        "        x = self.embedding(x).unsqueeze(1)\n",
        "\n",
        "        outputs, hidden = self.rnn(x, hidden)\n",
        "\n",
        "        predictions = self.fc(outputs.squeeze(1))\n",
        "\n",
        "        return predictions, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "\n",
        "        self.decoder = decoder\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        trg_vocab_size = self.decoder.embedding.embedding.num_embeddings\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        if isinstance(hidden, tuple):  # LSTM\n",
        "\n",
        "            hidden = (hidden[0][:self.decoder.rnn.num_layers], hidden[1][:self.decoder.rnn.num_layers])\n",
        "\n",
        "        else:  # RNN or GRU\n",
        "\n",
        "            hidden = hidden[:self.decoder.rnn.num_layers]\n",
        "\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            output, hidden = self.decoder(input, hidden)\n",
        "\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            input = trg[:, t] if random.random() < teacher_forcing_ratio else top1\n",
        "\n",
        "        return outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "ve60Xcp5fsK3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_word_accuracy(output, target):\n",
        "\n",
        "    pred_tokens = output.argmax(dim=2)\n",
        "\n",
        "    non_pad_elements = (target != char2index['<PAD>']).float()\n",
        "\n",
        "    correct = (pred_tokens == target).float() * non_pad_elements\n",
        "\n",
        "    accuracy = correct.sum() / non_pad_elements.sum()\n",
        "\n",
        "    return accuracy.item() * 100\n",
        "\n"
      ],
      "metadata": {
        "id": "RlbqiG2-gLYc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    epoch_acc = 0\n",
        "\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "\n",
        "        src = src.to(model.device)\n",
        "\n",
        "        trg = trg.to(model.device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        acc = calculate_word_accuracy(output.view(src.size(0), -1, output_dim), trg.view(src.size(0), -1))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        epoch_acc += acc\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    epoch_acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (src, trg) in enumerate(iterator):\n",
        "\n",
        "            src = src.to(model.device)\n",
        "\n",
        "            trg = trg.to(model.device)\n",
        "\n",
        "            output = model(src, trg)\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            acc = calculate_word_accuracy(output.view(src.size(0), -1, output_dim), trg.view(src.size(0), -1))\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            epoch_acc += acc\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "INPUT_DIMENSION = len(char2index)\n",
        "\n",
        "OUTPUT_DIMENSION = len(char2index)\n",
        "\n",
        "def train_model():\n",
        "\n",
        "    wandb.init(project=\"transliteration-Assign3\", config=sweep_config)\n",
        "\n",
        "    config = wandb.config\n",
        "\n",
        "    # Creating datasets and dataloaders\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, collate_fn=collate_fn)\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, collate_fn=collate_fn)\n",
        "\n",
        "    ENCODER_EMBEDDING_DIMENSION = config.embedding_dim\n",
        "\n",
        "    DECODER_EMBEDDING_DIMENSION = config.embedding_dim\n",
        "\n",
        "    HIDDEN_DIMENSION = config.hidden_dim\n",
        "\n",
        "    NUM_ENCODER_LAYERS = config.encoder_layers\n",
        "\n",
        "    NUM_DECODER_LAYERS = config.decoder_layers\n",
        "\n",
        "    RNN_CELL_TYPE = config.rnn_cell_type\n",
        "\n",
        "    BIDIRECTIONAL = config.bidirectional\n",
        "\n",
        "    DROPOUT_RATE = config.dropout_rate\n",
        "\n",
        "    GRADIENT_CLIP = config.gradient_clip\n",
        "\n",
        "    encoder = EncoderRNN(INPUT_DIMENSION, ENCODER_EMBEDDING_DIMENSION, HIDDEN_DIMENSION, NUM_ENCODER_LAYERS, DROPOUT_RATE, RNN_CELL_TYPE, BIDIRECTIONAL)\n",
        "\n",
        "    decoder = DecoderRNN(OUTPUT_DIMENSION, DECODER_EMBEDDING_DIMENSION, HIDDEN_DIMENSION, NUM_DECODER_LAYERS, DROPOUT_RATE, RNN_CELL_TYPE, BIDIRECTIONAL)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=char2index['<PAD>'])\n",
        "\n",
        "    wandb.watch(model, criterion, log=\"all\")\n",
        "\n",
        "    N_EPOCHS = config.num_epochs\n",
        "\n",
        "    CLIP = GRADIENT_CLIP\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    for epoch in range(N_EPOCHS):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "\n",
        "        valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "\n",
        "            best_valid_loss = valid_loss\n",
        "\n",
        "            torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "\n",
        "        # Logging hyperparameters and metrics\n",
        "\n",
        "        wandb.log({\n",
        "\n",
        "            \"train_loss\": train_loss,\n",
        "\n",
        "            \"train_acc\": train_acc,\n",
        "\n",
        "            \"valid_loss\": valid_loss,\n",
        "\n",
        "            \"valid_acc\": valid_acc,\n",
        "\n",
        "            \"input_dimension\": INPUT_DIMENSION,\n",
        "\n",
        "            \"output_dimension\": OUTPUT_DIMENSION,\n",
        "\n",
        "            \"encoder_embedding_dimension\": ENCODER_EMBEDDING_DIMENSION,\n",
        "\n",
        "            \"decoder_embedding_dimension\": DECODER_EMBEDDING_DIMENSION,\n",
        "\n",
        "            \"hidden_dimension\": HIDDEN_DIMENSION,\n",
        "\n",
        "            \"num_encoder_layers\": NUM_ENCODER_LAYERS,\n",
        "\n",
        "            \"num_decoder_layers\": NUM_DECODER_LAYERS,\n",
        "\n",
        "            \"rnn_cell_type\": RNN_CELL_TYPE,\n",
        "\n",
        "            \"bidirectional\": BIDIRECTIONAL,\n",
        "\n",
        "            \"dropout_rate\": DROPOUT_RATE,\n",
        "\n",
        "            \"gradient_clip\": CLIP\n",
        "\n",
        "        })\n",
        "\n",
        "        print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n",
        "\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc:.2f}%')\n",
        "\n",
        "    wandb.finish()\n"
      ],
      "metadata": {
        "id": "zPFM6xYFgF9x"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, function=train_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "TEAMIOBHgQp_",
        "outputId": "71b6ba69-a09b-4e10-b20a-8806bb87ee97"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3tscej4p with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_clip: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 150\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_cell_type: lstm\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mch23s020\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240517_075029-3tscej4p</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ch23s020/transliteration-Assign3/runs/3tscej4p' target=\"_blank\">visionary-sweep-1</a></strong> to <a href='https://wandb.ai/ch23s020/transliteration-Assign3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ch23s020/transliteration-Assign3/sweeps/e9i15px2' target=\"_blank\">https://wandb.ai/ch23s020/transliteration-Assign3/sweeps/e9i15px2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ch23s020/transliteration-Assign3' target=\"_blank\">https://wandb.ai/ch23s020/transliteration-Assign3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/ch23s020/transliteration-Assign3/sweeps/e9i15px2' target=\"_blank\">https://wandb.ai/ch23s020/transliteration-Assign3/sweeps/e9i15px2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ch23s020/transliteration-Assign3/runs/3tscej4p' target=\"_blank\">https://wandb.ai/ch23s020/transliteration-Assign3/runs/3tscej4p</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 86m 50s\n",
            "\tTrain Loss: 2.600 | Train Acc: 29.92%\n",
            "\t Val. Loss: 2.067 |  Val. Acc: 42.67%\n"
          ]
        }
      ]
    }
  ]
}